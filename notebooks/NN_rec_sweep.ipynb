{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install torchaudio-augmentations\n","!pip install audio_augmentations\n","!pip install wandb --upgrade\n","\n","import torch\n","import torchaudio\n","import torchtext\n","import torchaudio.functional as F\n","import torchaudio.transforms as T\n","from audio_augmentations import *\n","\n","import os, re, random\n","import numpy as np\n","import sklearn\n","import itertools\n","import time\n","\n","import pickle\n","from tqdm.auto import tqdm\n","from IPython.display import clear_output\n","import IPython.display as ipd\n","import gc\n","import matplotlib.pyplot as plt\n","import wandb\n","\n","print(torch.__version__)\n","print(torchaudio.__version__)\n","\n","import sys\n","\n","# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n","sys.path.append('/kaggle/working/Voice-commands-recognition')\n","sys.path.append('/kaggle/working/Voice-commands-recognition/notebooks')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!conda install -y gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!gdown --id 1yhD3dA8fmKncYaHmfbj4I0W640DLBeND"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!rm -rf ./Voice-commands-recognition\n","!git clone https://github.com/litvan007/Voice-commands-recognition.git\n","!mkdir /kaggle/working/Voice-commands-recognition/notebooks/signal_plots\n","!mkdir /kaggle/working/Voice-commands-recognition/notebooks/spec_plots\n","!mkdir /kaggle/working/Voice-commands-recognition/notebooks/mfcc_plots\n","!mkdir /kaggle/working/Voice-commands-recognition/checkpoints\n","!ls ./Voice-commands-recognition"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["path_to_zip_file = \"/kaggle/working/data.zip\"\n","directory_to_extract_to = \"./\"\n","\n","import zipfile\n","with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n","    zip_ref.extractall(directory_to_extract_to)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["random.seed(123456)\n","np.random.seed(123456)\n","torch.manual_seed(123456)\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data_path = '/kaggle/working/data'\n","data_list = []\n","with open(os.path.join(data_path, 'data_base_audio.pickle'), 'rb') as fh:\n","    data_list = pickle.load(fh)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(data_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def create_plots(signal, feature_map, name_fig, sample_rate=16000, \n","                            signal_plot_dir='signal_plots', \n","                            spec_plot_dir='spec_plots', \n","                            mfcc_plot_dir='mfcc_plots',\n","                            notebook_path='/kaggle/working/Voice-commands-recognition/notebooks'):\n","    \n","    signal = signal.numpy()\n","\n","    num_frames = signal.size\n","    time_axis = torch.arange(0, num_frames) / sample_rate\n","\n","    plt.plot(time_axis, signal, linewidth=1)\n","    plt.xlabel('Time')\n","    plt.title('Signal')\n","    plt.grid()\n","    plt.savefig(os.path.join(notebook_path, signal_plot_dir, name_fig))\n","    plt.clf()\n","\n","    plt.specgram(signal, Fs=sample_rate)\n","    plt.xlabel('Time')\n","    plt.title('Spectrogram')\n","    plt.savefig(os.path.join(notebook_path, spec_plot_dir, name_fig))\n","    plt.clf()\n","\n","    plt.imshow(feature_map, interpolation='nearest', origin='lower', aspect='auto')\n","    plt.xlabel('Frame')\n","    plt.title('MFCC')\n","    plt.savefig(os.path.join(notebook_path, mfcc_plot_dir, name_fig))\n","    plt.clf()\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["all_labels = set()\n","for example in data_list:\n","    all_labels.add(example['label'])\n","token_to_idx = {x: idx for idx, x in enumerate(all_labels)}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["token_to_idx"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lengths_subsets = {'train': int(0.8 * len(data_list)), 'valid': round(0.1 * len(data_list)), 'test': round(0.1 * len(data_list))}\n","train_vaild_subset, test_subset = torch.utils.data.random_split(data_list, \n","                                                                [lengths_subsets['train']+lengths_subsets['valid'], lengths_subsets['test']])\n","train_subset, valid_subset = torch.utils.data.random_split(train_vaild_subset, \n","                                                                [lengths_subsets['train'], lengths_subsets['valid']])\n","lengths_subsets"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Sound_dataset_commands(torch.utils.data.Dataset):\n","    def __init__(self, rootdir, subset, transform=None):\n","        self.transform = transform\n","        self.rootdir = rootdir\n","        self.subset = subset\n","        self.n_subset = len(self.subset)\n","        self.token_to_idx = {x: idx for idx, x in enumerate(all_labels)}\n","        self.idx_to_token = {idx: x for idx, x in enumerate(all_labels)}\n","\n","    def __getitem__(self, index):\n","        name, label = self.subset[index].values()\n","        signal, sample_rate = torchaudio.load(os.path.join(self.rootdir, name))\n","        signal = signal[0]\n","\n","        if self.transform:\n","            feature_map = self.transform(signal)\n","            signal = self.transform.transforms[0](signal)\n","        idx_label = self.token_to_idx[label]\n","\n","        return feature_map, idx_label, signal, name, label\n","\n","    def __len__(self):\n","        return len(self.subset)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def create_data_sets(n_mfcc):\n","    n_fft = 480\n","    win_length = None\n","    hop_length = 160\n","    mfcc_transform = T.MFCC(\n","        sample_rate=16000,\n","        n_mfcc=n_mfcc,\n","        melkwargs={\n","            \"n_fft\": n_fft,\n","            \"n_mels\": n_mfcc * 2,\n","            \"hop_length\": hop_length,\n","            \"f_min\": 20,\n","            \"f_max\": 4000\n","        },\n","    )\n","\n","    transforms = [\n","        RandomApply([Noise(min_snr=0.1, max_snr=0.3)], p=0.5),\n","        mfcc_transform\n","    ]\n","    transform = Compose(transforms=transforms)\n","    data_set = {\n","                'train': Sound_dataset_commands(rootdir=data_path, subset=train_subset, transform=transform),\n","                'valid': Sound_dataset_commands(rootdir=data_path, subset=valid_subset, transform=transform),\n","                'test': Sound_dataset_commands(rootdir=data_path, subset=test_subset, transform=transform)\n","            }\n","    return data_set"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def create_data_loaders(batch_size, data_set):\n","    def collate_fn(batch):\n","        X = torch.nn.utils.rnn.pad_sequence([sample[0].transpose(0, 1) for sample in batch], batch_first=True, padding_value=0).unsqueeze(1)\n","        y = torch.tensor([sample[1] for sample in batch])\n","\n","        signal = [sample[2] for sample in batch]\n","        name = [sample[3] for sample in batch]\n","        label = [sample[4] for sample in batch]\n","\n","        return X, y, signal, name, label\n","\n","    loaders = {\n","            'train': torch.utils.data.DataLoader(data_set['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn),\n","            'valid': torch.utils.data.DataLoader(data_set['valid'], batch_size=batch_size, shuffle=True, collate_fn=collate_fn),\n","            'test': torch.utils.data.DataLoader(data_set['test'], batch_size=1, shuffle=False, collate_fn=collate_fn)\n","            }\n","    return loaders"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from models.model import Speech_recognition_model\n","\n","def create_model_utils(params, weight_decay, amsgrad, max_lr, div_factor, epochs, steps_per_epoch):\n","    model = Speech_recognition_model(**params).to(device)\n","    criterion = torch.nn.CrossEntropyLoss().to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(),\n","                                lr=max_lr/div_factor,\n","                                weight_decay=weight_decay,\n","                                amsgrad=amsgrad)\n","\n","    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr,\n","                            steps_per_epoch=steps_per_epoch,\n","                            epochs=epochs, div_factor=div_factor)\n","    \n","    return model, criterion, optimizer, scheduler\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_accuracy(y_pred, y_test):\n","    correct_results_sum = (y_pred == y_test).sum().float()\n","    acc = correct_results_sum/(y_test.size(0))\n","    \n","    return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from models.model import Speech_recognition_model\n","import yaml\n","\n","# Сделать разные конфиги\n","params = None\n","with open(\"/kaggle/working/Voice-commands-recognition/configs/model_params.yaml\", \"r\") as stream:\n","    try:\n","        params = yaml.safe_load(stream)\n","    except yaml.YAMLError as exc:\n","        print(exc)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["params"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sweep_configuration = {\n","    'method': 'bayes',\n","        'metric': {\n","        'goal': 'maximize', \n","        'name': 'valid_accuracy'\n","        },\n","    'parameters': \n","        {  \"n_mfcc\": {\"values\": [24, 36, 48, 60]},\n","           \"CNN_out_channels\": {'values': [32, 64]}, \"CNN_kernel_size\": {'values': [3, 5]},\n","            \"ResCNN_kernel_size\": {'values': [3, 5]}, \"ResCNN_dropout\": {'min': 0.1, 'max': 0.6}, \"ResCNN_n_cnn_layers\": {'min': 1, \"max\": 5},\n","            \"FC_out_features\": {\"values\": [128, 256, 512, 1024]},\n","            \"RNN_num_layers\": {'min': 2, 'max': 6}, \"RNN_bi\": {'values': [True]}, \"RNN_dropout\": {'min': 0.1, 'max': 0.5},\n","            \"Classifier_dropout\": {'min': 0.1, 'max': 0.5},\n","            'weight_decay': {'min': 0.0001, 'max': 0.0007}, 'amsgrad': {'values': [False, True]}, 'max_lr': {'min': 0.01, 'max': 0.1}, 'div_factor': {'min': 100, 'max': 300}, 'max_norm': {'min': 0.7, 'max': 1.5}, \n","            'epochs': {'values': [10]}, 'batch_size': {'values': [128]}\n","        }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sweep_configuration_test_best = {\n","    'method': 'bayes',\n","        'metric': {\n","        'goal': 'maximize', \n","        'name': 'valid_accuracy'\n","        },\n","    'parameters': \n","        {  \"n_mfcc\": {\"values\": [36]},\n","           \"CNN_out_channels\": {'values': [64]}, \"CNN_kernel_size\": {'values': [3]},\n","            \"ResCNN_kernel_size\": {'values': [3]}, \"ResCNN_dropout\":  {'values': [0.2557663304729153]}, \"ResCNN_n_cnn_layers\": {'values': [2]},\n","            \"FC_out_features\": {\"values\": [1024]},\n","            \"RNN_num_layers\": {'values': [2]}, \"RNN_bi\": {'values': [True]}, \"RNN_dropout\": {'values': [0.27401574970155235]},\n","            \"Classifier_dropout\": {'values': [0.451876615914206]},\n","            'weight_decay': {'values': [0.0005163181608617645]}, 'amsgrad': {'values': [False]}, 'max_lr': {'values': [0.06894893452161045]}, 'div_factor': {'values': [124]}, 'max_norm': {'values': [1.4018824914643906]}, \n","            'epochs': {'values': [10]}, 'batch_size': {'values': [64]}\n","        }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sweep_id = wandb.sweep(sweep=sweep_configuration_test_best, project=\"Commands Recognition\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["signal_plot_dir = 'signal_plots'\n","spec_plot_dir = 'spec_plots'\n","mfcc_plot_dir = 'mfcc_plots'\n","notebook_path = '/kaggle/working/Voice-commands-recognition/notebooks'\n","\n","def sweep_func():\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    \n","    wandb.init(project=\"Commands Recognition\")\n","    columns = [\"name\", \"song_file\", \"signal_plot\", \"spec_plot\", \"mfcc_plot\", \"pred_label\", \"true_label\"]\n","    valid_table = wandb.Table(columns=columns)\n","    data_set = create_data_sets(wandb.config['n_mfcc'])\n","    loaders = create_data_loaders(wandb.config['batch_size'], data_set)\n","    epochs = wandb.config['epochs']\n","    max_norm = wandb.config['max_norm']\n","    params['Architecture']['CNN_params']['kernel_size'] = wandb.config['CNN_kernel_size']\n","    params['Architecture']['CNN_params']['padding'] = wandb.config['CNN_kernel_size'] // 2\n","    params['Architecture']['CNN_params']['out_channels'] = wandb.config['CNN_out_channels']\n","    params['Architecture']['ResCNN_params']['in_channels'] = wandb.config['CNN_out_channels']\n","    params['Architecture']['ResCNN_params']['out_channels'] = wandb.config['CNN_out_channels']\n","    params['Architecture']['ResCNN_params']['kernel_size'] = wandb.config['ResCNN_kernel_size']\n","    params['Architecture']['ResCNN_params']['padding'] = wandb.config['ResCNN_kernel_size'] // 2\n","    params['Architecture']['ResCNN_params']['dropout'] = wandb.config['ResCNN_dropout'] \n","    params['Architecture']['ResCNN_params']['n_cnn_layers'] = wandb.config['ResCNN_n_cnn_layers']\n","    params['Architecture']['ResCNN_params']['n_feats'] = wandb.config['n_mfcc'] // 2\n","    params['Architecture']['Fully_connected_params']['in_features'] = wandb.config['CNN_out_channels'] * params['Architecture']['ResCNN_params']['n_feats']\n","    params['Architecture']['Fully_connected_params']['out_features'] = wandb.config['FC_out_features']\n","    params['Architecture']['RNN_params']['input_size'] = wandb.config['FC_out_features']\n","    params['Architecture']['RNN_params']['hidden_size'] = wandb.config['FC_out_features']\n","    params['Architecture']['RNN_params']['num_layers'] = wandb.config['RNN_num_layers']\n","    params['Architecture']['RNN_params']['bidirectional'] = wandb.config['RNN_bi']\n","    params['Architecture']['RNN_params']['dropout'] = wandb.config['RNN_dropout']\n","    params['Architecture']['Attention_params']['feature_dim'] = wandb.config['FC_out_features']\n","    params['Architecture']['Attention_params']['step_dim'] = params['Architecture']['RNN_params']['num_layers'] * 2\n","    params['Architecture']['Classifier_params']['in_features'] = wandb.config['FC_out_features']\n","    params['Architecture']['Classifier_params']['dropout'] = wandb.config['Classifier_dropout']\n","    params['Architecture']['Classifier_params']['out_features'] = params['Architecture']['Classifier_params']['in_features'] // 2\n","    params['Settings']['Other']['batch_size'] = wandb.config['batch_size']\n","\n","\n","    model, criterion, optimizer, scheduler = create_model_utils(params['Architecture'],\n","                                                                wandb.config['weight_decay'],\n","                                                                wandb.config['amsgrad'],\n","                                                                wandb.config['max_lr'],\n","                                                                wandb.config['div_factor'],\n","                                                                epochs,\n","                                                                len(loaders['train']))\n","    \n","\n","    print(params)\n","    def run_one_epoch(epoch, cross_valid=False, print_freq=1):\n","        start = time.time()\n","        total_loss = 0\n","        total_accuracy = 0\n","\n","        data_loader = loaders['train'] if not cross_valid else loaders['valid']\n","\n","        for i, (data) in enumerate(tqdm(data_loader)):\n","            feature_map, idx_label, signal, name, label = data\n","            input_data = feature_map.to(device)\n","            target_labels = idx_label.to(device)\n","\n","            output_logits = model(input_data)\n","            loss = criterion(output_logits, target_labels)\n","            if not cross_valid:\n","                optimizer.zero_grad()\n","                loss.backward()\n","                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),\n","                                                            max_norm)\n","                optimizer.step()\n","\n","            total_loss += loss.item()\n","            pred_labels = torch.round(torch.sigmoid(output_logits)).type(torch.int8).argmax(1)\n","            accuracy = get_accuracy(pred_labels, target_labels)\n","            total_accuracy += accuracy\n","            optim_state = optimizer.state_dict()\n","            curr_lr = optim_state['param_groups'][0]['lr']\n","            \n","            if i % print_freq == 0:\n","                    print('Epoch {0} | Iter {1} | Average Loss {2:.3f} | '\n","                        'Current Loss {3:.6f} | Current accuracy {4:.6f} | Current lr {5:.5f} | {6:.1f} ms/batch'.format(\n","                            epoch + 1, i + 1, total_loss / (i + 1),\n","                            loss.item(), accuracy, curr_lr, 1000 * (time.time() - start) / (i + 1)),\n","                        flush=True)\n","\n","            if cross_valid:\n","                name_fig = f'{name[0].replace(\"/\", \"_\").split(\".\")[0]}.png'\n","                create_plots(signal[0], feature_map[0][0], name_fig)    \n","                temp = [name[0], \n","                    wandb.Audio(signal[0], sample_rate=16000), \n","                    wandb.Image(os.path.join( notebook_path, signal_plot_dir, name_fig )),\n","                    wandb.Image(os.path.join( notebook_path, spec_plot_dir, name_fig )),\n","                    wandb.Image(os.path.join( notebook_path, mfcc_plot_dir, name_fig )),\n","                    pred_labels[0],\n","                    target_labels[0]]\n","                valid_table.add_row(*temp)\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            \n","        return total_loss / (i + 1), total_accuracy / (i + 1)\n","\n","    epoch = 0\n","    checkpoint = False\n","    visdom = True\n","    save_folder = '/kaggle/working/Voice-commands-recognition/checkpoints'\n","\n","    tr_loss = []\n","    cv_loss = []\n","    tr_acc = []\n","    cv_acc = []\n","    best_val_loss = 0.1\n","\n","    wandb.watch(model, log_freq=1, log='all')\n","    for epoch in tqdm(np.arange(epoch, epochs)):\n","        print(\"Training...\")\n","        model.train()\n","        start = time.time()\n","        tr_avg_loss, tr_avg_acc = run_one_epoch(epoch)\n","        tr_loss.append(tr_avg_loss)\n","        tr_acc.append(tr_avg_acc)\n","\n","        print('-' * 85)\n","        print('Train Summary | End of Epoch {0} | Time {1:.2f}s | '\n","                    'Train Loss {2:.3f} | Accuracy {3:.3f}'.format(\n","                        epoch + 1, time.time() - start, tr_avg_loss, tr_avg_acc))\n","        print('-' * 85)\n","\n","        if checkpoint:\n","            file_path = os.path.join(\n","            save_folder, 'epoch_{0}_loss_{1:.4f}.pth.tar'.format(epoch + 1, cv_avg_loss))\n","            torch.save(model.serialize(model, optimizer, scheduler, epoch + 1,\n","                                            tr_loss=tr_loss,\n","                                            cv_loss=cv_loss),\n","                    file_path)\n","            print('Saving checkpoint model to %s' % file_path)\n","\n","        print('Cross validation...')\n","        model.eval()  # Turn off Batchnorm & Dropout\n","        cv_avg_loss, cv_avg_acc = run_one_epoch(epoch, cross_valid=True)\n","        cv_loss.append(cv_avg_loss)\n","        cv_loss.append(cv_avg_acc)\n","\n","        print('-' * 185)\n","        print('Valid Summary | End of Epoch {0} | Time {1:.2f}s | '\n","                'Valid Loss {2:.3f} | Accuracy {3:.3f}'.format(\n","                    epoch + 1, time.time() - start, cv_avg_loss, cv_avg_acc))\n","        print('-' * 185)\n","\n","        # Save the best model\n","        if cv_avg_loss < best_val_loss:\n","            best_val_loss = cv_avg_loss\n","            model_path = 'epoch_{0}_loss_{1:.4f}_best.pth.tar'.format(epoch + 1, cv_avg_loss)\n","            file_path = os.path.join(save_folder, model_path)\n","            torch.save(model.serialize(model, optimizer, scheduler, epoch + 1,\n","                                        tr_loss=tr_loss,\n","                                        cv_loss=cv_loss),\n","                    file_path)\n","            print(\"Find better validated model, saving to %s\" % file_path)\n","        \n","\n","        wandb.log({\"epoch\": epoch, \"train_loss\": tr_avg_loss})\n","        wandb.log({\"epoch\": epoch, \"valid_loss\": cv_avg_loss})\n","        wandb.log({\"epoch\": epoch, \"train_accuracy\": tr_avg_acc})\n","        wandb.log({\"epoch\": epoch, \"valid_accuracy\": cv_avg_acc})\n","        wandb.run.log({\"valid_inference\" : valid_table}) "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["wandb.agent(sweep_id, function=sweep_func, count=10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"vscode":{"interpreter":{"hash":"f5fb8b2ac1159a8f54b57f028b1e7265b953cba3f174d2e42f0525a72ffdb534"}}},"nbformat":4,"nbformat_minor":4}
